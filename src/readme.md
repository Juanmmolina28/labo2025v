# Entrega_Final_Molina_Chiavassa: Pipeline de An√°lisis de Churn (LightGBM)

Este repositorio contiene el flujo de trabajo (workflow/pipeline) implementado para la competencia de la Maestr√≠a, 
cuyo objetivo es predecir la salida de clientes (*churn*) en una entidad financiera y optimizar la ganancia. 
El modelo central se basa en LightGBM, utilizando Feature Engineering hist√≥rico (medias m√≥viles exponenciales - EMA) y 
optimizaci√≥n bayesiana de hiperpar√°metros.

## üõ†Ô∏è Entorno de Ejecuci√≥n

El script est√° dise√±ado para correr en una m√°quina virtual con **R runtime**.

* **M√°quina Recomendada:** `desktop-jr`
    * **Caracter√≠sticas:** 64 GB de RAM, 8 vCPU.
    * **Ubicaci√≥n del Datacenter:** S√£o Paulo, Brasil.
* **Lenguaje:** **R** (Aseg√∫rese de cambiar el tipo de entorno de ejecuci√≥n/runtime a R si usa Google Colab o Jupyter con kernel R).

---

## ‚öôÔ∏è Par√°metros Clave (Ajustables)

Los siguientes par√°metros est√°n definidos en el c√≥digo y son cr√≠ticos para la reproducibilidad del experimento:

| Par√°metro | Valor por defecto | Descripci√≥n | Bloque de C√≥digo |
| :--- | :--- | :--- | :--- |
| **`PARAM$semilla_primigenia`** | `292267` | Semilla de aleatoriedad principal para la reproducibilidad. | *Inicializaci√≥n* |
| **`PARAM$CA$metodo`** | `"MachineLearning"` | M√©todo para el Catastrophe Analysis (asignar NA a variables con valores cero). | *Corregir_Rotas* |
| **`PARAM$DR$metodo`** | `"deflacion"` | M√©todo para corregir el Data Drifting (ajuste de variables monetarias). | *Drift* |
| **`PARAM$FE_rf$arbolitos`** | `20` | N√∫mero de √°rboles para el Feature Engineering de Random Forest. | *FE_rf* |
| **`PARAM$trainingstrategy$training_pct`**| `0.4` | Porcentaje de *undersampling* aplicado a la clase **CONTINUA** en el training set (para velocidad). | *Training Strategy* |
| **`PARAM$hipeparametertuning$num_interations`**| `50` | Cantidad de iteraciones de la **Optimizaci√≥n Bayesiana**. | *Hyperparameter Tuning* |
| **`PARAM$kaggle$cortes`** | `seq(1800, 2400, by = 100)` | Rangos de cortes de probabilidad (env√≠os) para generar los archivos de Kaggle. | *Kaggle Submit* |

---

## üöÄ Instrucciones de Corrida

Para ejecutar y reproducir este experimento, siga los siguientes pasos:

### 1. Entorno de R y Librer√≠as

1.  Aseg√∫rese de que el *runtime* de su entorno (Colab, Jupyter, etc.) est√© configurado en **R**.
2.  Ejecute las celdas de inicializaci√≥n para limpiar la memoria y cargar las librer√≠as necesarias (`data.table`, `R.utils`, `mice`, `lightgbm`, `DiceKriging`, `mlrMBO`, `yaml`). El script instalar√° autom√°ticamente las que falten.

### 2. Preprocesamiento (Celdas 61 a 95)

1.  **Carga y Catastrophe Analysis (CA):**
    * Ejecute las celdas que cargan el *dataset* y aplican el `Catastrophe Analysis` para manejar los datos rotos (ceros generalizados).

2.  **Data Drifting (DR):**
    * Ejecute la celda que crea la tabla de √≠ndices (`tb_indices`) con `IPC`, `dolar_blue`, `dolar_oficial` y `UVA`.
    * El m√©todo actual es **`deflacion`** (ajuste por IPC), aplicado a todos los campos monetarios (`m*` variables).

3.  **Feature Engineering (FEhist):**
    * Esta es la etapa m√°s rica, donde se crean las **medias m√≥viles exponenciales (EMA)** con ventanas de 3, 6 y 12 meses, y las **razones (ratios)** de estas.
    * Ejecute las celdas del bloque `FEhist` para generar las nuevas variables hist√≥ricas.

### 3. Modelado y Optimizaci√≥n (Celdas 99 a 107)

1.  **Training Setup:**
    * Ejecute las celdas que definen los conjuntos de *training* (`foto_mes` hasta `202105` con *undersampling* de 40% de la clase `CONTINUA`) y *validation* (`202107`).

2.  **Optimizaci√≥n Bayesiana (Hyperparameter Tuning):**
    * Ejecute el bloque que inicia la `mbo()`. Este proceso puede tardar, ya que realiza `50` iteraciones (o m√°s, seg√∫n se modifique el par√°metro `PARAM$hipeparametertuning$num_interations`) para encontrar los mejores hiperpar√°metros de LightGBM (optimizando el **AUC** en el set de *validation*).

### 4. Producci√≥n y Entrega (Celdas posteriores)

1.  **Final Training:**
    * Una vez terminada la optimizaci√≥n bayesiana, el script extrae los mejores hiperpar√°metros (`PARAM$out$lgbm$mejores_hiperparametros`) y los utiliza para entrenar el **`final_model`** sobre todo el *final\_train* set (hasta `202107`) sin *undersampling*.

2.  **Scoring y Kaggle Submit:**
    * El modelo final se aplica al set de **futuro** (`202109`).
    * Se generan los archivos de env√≠o (`KAxxxxxx_yyyy.csv`) para Kaggle en este caso se deja solo con 2100 env√≠os ya que se trata de la mejor alternativa evaluada.
